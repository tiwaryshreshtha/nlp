{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1 Word analysis"
      ],
      "metadata": {
        "id": "0naIPPi2Lu1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Sample text for analysis\n",
        "text = \"Natural Language Processing is a fascinating field of study.\"\n",
        "# Process the text with spaCy\n",
        "doc = nlp(text)\n",
        "# Extracting tokens and lemmatization\n",
        "tokens = [token.text for token in doc]\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Lemmas:\", lemmas)\n",
        "# Dependency parsing\n",
        "print(\"\\nDependency Parsing:\")\n",
        "for token in doc:\n",
        " print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
        " [child for child in token.children])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oREdrNMEL1wr",
        "outputId": "87bfa9ae-82f6-4940-dfa6-d81084ee91b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'study', '.']\n",
            "Lemmas: ['Natural', 'Language', 'Processing', 'be', 'a', 'fascinating', 'field', 'of', 'study', '.']\n",
            "\n",
            "Dependency Parsing:\n",
            "Natural compound Language PROPN []\n",
            "Language compound Processing PROPN [Natural]\n",
            "Processing nsubj is AUX [Language]\n",
            "is ROOT is AUX [Processing, field, .]\n",
            "a det field NOUN []\n",
            "fascinating amod field NOUN []\n",
            "field attr is AUX [a, fascinating, of]\n",
            "of prep field NOUN [study]\n",
            "study pobj of ADP []\n",
            ". punct is AUX []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 : Case study"
      ],
      "metadata": {
        "id": "yNAo7Y4OL520"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "# Sample customer feedback data\n",
        "customer_feedback = [\n",
        " \"The product is amazing! I love the quality.\",\n",
        " \"The customer service was terrible, very disappointed.\",\n",
        " \"Great experience overall, highly recommended.\",\n",
        " \"The delivery was late, very frustrating.\"\n",
        "]\n",
        "def analyze_feedback(feedback):\n",
        " for idx, text in enumerate(feedback, start=1):\n",
        "  print(f\"\\nAnalyzing Feedback {idx}: '{text}'\")\n",
        "  doc = nlp(text)\n",
        "  # Extract tokens and lemmatization\n",
        "  tokens = [token.text for token in doc]\n",
        "  lemmas = [token.lemma_ for token in doc]\n",
        "  print(\"Tokens:\", tokens)\n",
        "  print(\"Lemmas:\", lemmas)\n",
        "  # Dependency parsing\n",
        "  print(\"\\nDependency Parsing:\")\n",
        "  for token in doc:\n",
        "   print(token.text, token.dep_, token.head.text, token.head.pos_,[child for child in token.children])\n",
        "if __name__ == \"__main__\":\n",
        " analyze_feedback(customer_feedback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkaR2jnPL8Xd",
        "outputId": "b400a521-23f0-45f3-852c-7f45b4d89422"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing Feedback 1: 'The product is amazing! I love the quality.'\n",
            "Tokens: ['The', 'product', 'is', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']\n",
            "Lemmas: ['the', 'product', 'be', 'amazing', '!', 'I', 'love', 'the', 'quality', '.']\n",
            "\n",
            "Dependency Parsing:\n",
            "The det product NOUN []\n",
            "product nsubj is AUX [The]\n",
            "is ROOT is AUX [product, amazing, !]\n",
            "amazing acomp is AUX []\n",
            "! punct is AUX []\n",
            "I nsubj love VERB []\n",
            "love ROOT love VERB [I, quality, .]\n",
            "the det quality NOUN []\n",
            "quality dobj love VERB [the]\n",
            ". punct love VERB []\n",
            "\n",
            "Analyzing Feedback 2: 'The customer service was terrible, very disappointed.'\n",
            "Tokens: ['The', 'customer', 'service', 'was', 'terrible', ',', 'very', 'disappointed', '.']\n",
            "Lemmas: ['the', 'customer', 'service', 'be', 'terrible', ',', 'very', 'disappointed', '.']\n",
            "\n",
            "Dependency Parsing:\n",
            "The det service NOUN []\n",
            "customer compound service NOUN []\n",
            "service nsubj was AUX [The, customer]\n",
            "was ROOT was AUX [service, disappointed, .]\n",
            "terrible amod disappointed ADJ []\n",
            ", punct disappointed ADJ []\n",
            "very advmod disappointed ADJ []\n",
            "disappointed acomp was AUX [terrible, ,, very]\n",
            ". punct was AUX []\n",
            "\n",
            "Analyzing Feedback 3: 'Great experience overall, highly recommended.'\n",
            "Tokens: ['Great', 'experience', 'overall', ',', 'highly', 'recommended', '.']\n",
            "Lemmas: ['great', 'experience', 'overall', ',', 'highly', 'recommend', '.']\n",
            "\n",
            "Dependency Parsing:\n",
            "Great amod experience NOUN []\n",
            "experience nsubj recommended VERB [Great]\n",
            "overall advmod recommended VERB []\n",
            ", punct recommended VERB []\n",
            "highly advmod recommended VERB []\n",
            "recommended ROOT recommended VERB [experience, overall, ,, highly, .]\n",
            ". punct recommended VERB []\n",
            "\n",
            "Analyzing Feedback 4: 'The delivery was late, very frustrating.'\n",
            "Tokens: ['The', 'delivery', 'was', 'late', ',', 'very', 'frustrating', '.']\n",
            "Lemmas: ['the', 'delivery', 'be', 'late', ',', 'very', 'frustrating', '.']\n",
            "\n",
            "Dependency Parsing:\n",
            "The det delivery NOUN []\n",
            "delivery nsubj was AUX [The]\n",
            "was ROOT was AUX [delivery, frustrating, .]\n",
            "late advmod frustrating ADJ []\n",
            ", punct frustrating ADJ []\n",
            "very advmod frustrating ADJ []\n",
            "frustrating acomp was AUX [late, ,, very]\n",
            ". punct was AUX []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Word generation"
      ],
      "metadata": {
        "id": "ALT96yi_Mzbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "import random\n",
        "# Download NLTK resources (run only once if not downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "# Load a corpus (for example, the Gutenberg corpus)\n",
        "words = nltk.corpus.gutenberg.words()\n",
        "# Create a bigram model\n",
        "bigrams = list(nltk.bigrams(words))\n",
        "# Choose a starting word (you can choose any word from the corpus)\n",
        "starting_word = \"the\"\n",
        "generated_text = [starting_word]\n",
        "# Generate 20 words of text\n",
        "for _ in range(20):\n",
        " # Get all bigrams that start with the last generated word\n",
        " possible_words = [word2 for (word1, word2) in bigrams if word1.lower() == generated_text[-1].lower()]\n",
        "\n",
        " # Choose a word randomly from the possible options\n",
        " next_word = random.choice(possible_words)\n",
        " generated_text.append(next_word)\n",
        "# Print the generated text\n",
        "print(' '.join(generated_text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9y5QxCaTM3zP",
        "outputId": "b72ef3cc-83c1-43ae-de15-c822db06df74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the work of cedar and colossal orbs hath chosen men . Elton for Peleg .\" \" I absented myself to him\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 : Case Study"
      ],
      "metadata": {
        "id": "RoJsU0OlNP7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "class EmailAutocompleteSystem:\n",
        "    def __init__(self):\n",
        "        self.model_name = \"gpt2\"\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
        "    def generate_suggestions(self, user_input, context):\n",
        "        input_text = f\"{context} {user_input}\"\n",
        "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "        with torch.no_grad():\n",
        "            output = self.model.generate(input_ids, max_length=50, num_return_sequences=1,  no_repeat_ngram_size=2)\n",
        "            generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "            suggestions = generated_text.split()[len(user_input.split()):]\n",
        "        return suggestions\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    autocomplete_system = EmailAutocompleteSystem()\n",
        "    # Assume user is composing an email with some context\n",
        "    email_context = \"Subject: Discussing Project Proposal\\nHi [Recipient],\"\n",
        "    while True:\n",
        "        user_input = input(\"Enter your sentence (type 'exit' to end): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        suggestions = autocomplete_system.generate_suggestions(user_input, email_context)\n",
        "        if suggestions:\n",
        "            print(\"Autocomplete Suggestions:\", suggestions)\n",
        "        else:\n",
        "            print(\"No suggestions available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmQJPTx9NRb7",
        "outputId": "8cade133-6fd7-4da6-c61b-3977d577e336"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Enter your sentence (type 'exit' to end): hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocomplete Suggestions: ['Discussing', 'Project', 'Proposal', 'Hi', '[Recipient],', 'hi.', \"I'm\", 'a', 'developer', 'at', 'a', 'company', 'that', 'is', 'working', 'on', 'a', 'project', 'called', 'Project', 'PROJECT.', \"It's\", 'a', 'small', 'project', 'that', \"I've\", 'been', 'working', 'with', 'for', 'a', 'while.', 'The', 'project']\n",
            "Enter your sentence (type 'exit' to end): yellow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocomplete Suggestions: ['Discussing', 'Project', 'Proposal', 'Hi', '[Recipient],', 'yellowcake,', \"I'm\", 'interested', 'in', 'your', 'proposal', 'to', 'create', 'a', 'new', 'project', 'that', 'will', 'be', 'able', 'to', 'be', 'used', 'as', 'a', 'testnet', 'for', 'the', 'new', 'version', 'of', 'the', 'project.', \"I've\", 'been', 'working']\n",
            "Enter your sentence (type 'exit' to end): its okay \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Autocomplete Suggestions: ['Project', 'Proposal', 'Hi', '[Recipient],', 'its', 'okay', 'ive', 'been', 'thinking', 'about', 'this', 'for', 'a', 'while.', \"I'm\", 'not', 'sure', 'if', 'I', 'can', 'answer', 'your', 'question,', 'but', 'I', 'think', \"it's\", 'important', 'to', 'know', 'what', \"you're\", 'talking', 'about.']\n",
            "Enter your sentence (type 'exit' to end): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Text Classification"
      ],
      "metadata": {
        "id": "3uNS2JONPbI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install scikit-learn if not already installed\n",
        "!pip install scikit-learn\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['sci.med', 'sci.space', 'comp.graphics', 'talk.politics.mideast']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "# Split the data into training and testing sets\n",
        "X_train = newsgroups_train.data\n",
        "X_test = newsgroups_test.data\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "# Create a pipeline with TF-IDF vectorizer and LinearSVC classifier\n",
        "model = make_pipeline(\n",
        " TfidfVectorizer(),\n",
        " LinearSVC()\n",
        ")\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "# Predict labels for the test set\n",
        "predictions = model.predict(X_test)\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE3ncrE9Pf7H",
        "outputId": "295f15d5-d7b1-4e01-f0b6-b44c7ad6ee4d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Accuracy: 0.9504823151125402\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.97      0.93       389\n",
            "           1       0.96      0.91      0.94       396\n",
            "           2       0.98      0.94      0.96       394\n",
            "           3       0.98      0.98      0.98       376\n",
            "\n",
            "    accuracy                           0.95      1555\n",
            "   macro avg       0.95      0.95      0.95      1555\n",
            "weighted avg       0.95      0.95      0.95      1555\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 : Case Study"
      ],
      "metadata": {
        "id": "Cizue2Y4PqJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "# Load the 20 Newsgroups dataset as a proxy for customer support emails\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=['comp.sys.ibm.pc.hardware',\n",
        "'comp.sys.mac.hardware', 'rec.autos', 'rec.motorcycles', 'sci.electronics'])\n",
        "# Prepare data and target labels\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Create TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000)\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "# Train the LinearSVC classifier\n",
        "classifier = LinearSVC()\n",
        "classifier.fit(X_train, y_train)\n",
        "# Predict labels for the test set\n",
        "predictions = classifier.predict(X_test)\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, predictions, target_names=newsgroups.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTEo5CEqPruN",
        "outputId": "3f522514-eca1-49b3-94f9-74ca83092f42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9389623601220752\n",
            "\n",
            "Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "comp.sys.ibm.pc.hardware       0.92      0.91      0.91       212\n",
            "   comp.sys.mac.hardware       0.94      0.93      0.94       198\n",
            "               rec.autos       0.97      0.93      0.95       179\n",
            "         rec.motorcycles       0.96      0.99      0.97       205\n",
            "         sci.electronics       0.92      0.93      0.92       189\n",
            "\n",
            "                accuracy                           0.94       983\n",
            "               macro avg       0.94      0.94      0.94       983\n",
            "            weighted avg       0.94      0.94      0.94       983\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Semantic Analysis"
      ],
      "metadata": {
        "id": "FvKx72M7PzUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "# Import required libraries\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download pre-trained word vectors (Word2Vec)\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "\"Natural language processing is a challenging but fascinating field.\",\n",
        "\"Word embeddings capture semantic meanings of words in a vector space.\"\n",
        "]\n",
        "# Tokenize sentences\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "# Perform semantic analysis using pre-trained word vectors\n",
        "for tokenized_sentence in tokenized_sentences:\n",
        " for word in tokenized_sentence:\n",
        "  if word in word_vectors:\n",
        "   similar_words = word_vectors.most_similar(word)\n",
        "   print(f\"Words similar to '{word}': {similar_words}\")\n",
        "  else:\n",
        "   print(f\"'{word}' is not in the pre-trained Word2Vec model.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7Saesh8QCmW",
        "outputId": "5d0e3d39-3e6b-477f-d41a-10c150265b65"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "Words similar to 'natural': [('Splittorff_lacked', 0.636509358882904), ('Natural', 0.58078932762146), ('Mike_Taugher_covers', 0.577259361743927), ('manmade', 0.5276211500167847), ('shell_salted_pistachios', 0.5084421634674072), ('unnatural', 0.5030758380889893), ('naturally', 0.49992606043815613), ('Intraparty_squabbles', 0.4988228678703308), ('Burt_Bees_®', 0.49539363384246826), ('causes_Buxeda', 0.4935200810432434)]\n",
            "Words similar to 'language': [('langauge', 0.7476695775985718), ('Language', 0.6695356369018555), ('languages', 0.6341332197189331), ('English', 0.6120712757110596), ('CMPB_Spanish', 0.6083104610443115), ('nonnative_speakers', 0.6063109636306763), ('idiomatic_expressions', 0.5889801979064941), ('verb_tenses', 0.58415687084198), ('Kumeyaay_Diegueno', 0.5798824429512024), ('dialect', 0.5724600553512573)]\n",
            "Words similar to 'processing': [('Processing', 0.7285515666007996), ('processed', 0.6519132852554321), ('processor', 0.636760413646698), ('warden_Dominick_DeRose', 0.6166526675224304), ('processors', 0.5953895449638367), ('Discoverer_Enterprise_resumed', 0.5376213192939758), ('LSI_Tarari', 0.520267903804779), ('processer', 0.5166687369346619), ('remittance_processing', 0.5144169926643372), ('Farmland_Foods_pork', 0.5071728825569153)]\n",
            "Words similar to 'is': [('was', 0.6549733281135559), (\"isn'ta\", 0.6439523100852966), ('seems', 0.634029746055603), ('Is', 0.6085968613624573), ('becomes', 0.5841935276985168), ('appears', 0.5822900533676147), ('remains', 0.5796942114830017), ('іѕ', 0.5695518255233765), ('makes', 0.5567088723182678), ('isn_`_t', 0.5513144135475159)]\n",
            "'a' is not in the pre-trained Word2Vec model.\n",
            "Words similar to 'challenging': [('difficult', 0.6388775110244751), ('challenge', 0.5953003764152527), ('daunting', 0.569800615310669), ('tough', 0.5689979791641235), ('challenges', 0.5471934676170349), ('challenged', 0.5449535846710205), ('Challenging', 0.5242965817451477), ('tricky', 0.5236554741859436), ('toughest', 0.5169045329093933), ('diffi_cult', 0.5010539889335632)]\n",
            "Words similar to 'but': [('although', 0.8104525804519653), ('though', 0.7285684943199158), ('because', 0.7225914597511292), ('so', 0.6865807771682739), ('But', 0.6826984882354736), ('Although', 0.6188263297080994), ('Though', 0.6153667569160461), ('Unfortunately', 0.6031029224395752), ('Of_course', 0.593142032623291), ('anyway', 0.5869061350822449)]\n",
            "Words similar to 'fascinating': [('interesting', 0.7623067498207092), ('intriguing', 0.7245113253593445), ('enlightening', 0.6644250154495239), ('captivating', 0.6459898352622986), ('facinating', 0.6416683793067932), ('riveting', 0.6324825286865234), ('instructive', 0.6210989356040955), ('endlessly_fascinating', 0.6188612580299377), ('revelatory', 0.6170244216918945), ('engrossing', 0.6126049160957336)]\n",
            "Words similar to 'field': [('fields', 0.5582526326179504), ('fi_eld', 0.5188260078430176), ('Keith_Toogood', 0.49749255180358887), ('Mackenzie_Hoambrecker', 0.49514278769493103), ('Josh_Arauco_kicked', 0.48817265033721924), ('Nick_Cattoi', 0.4863145053386688), ('Armando_Cuko', 0.4853871166706085), ('Jon_Striefsky', 0.48322004079818726), ('kicker_Nico_Grasu', 0.47572532296180725), ('Chris_Manfredini_kicked', 0.47327715158462524)]\n",
            "'.' is not in the pre-trained Word2Vec model.\n",
            "Words similar to 'word': [('phrase', 0.6777030825614929), ('words', 0.5864380598068237), ('verb', 0.5517287254333496), ('Word', 0.54575115442276), ('adjective', 0.5290762186050415), ('cuss_word', 0.5272089242935181), ('colloquialism', 0.5160348415374756), ('noun', 0.5129537582397461), ('astrology_#/##/##', 0.5039082765579224), ('synonym', 0.49379870295524597)]\n",
            "'embeddings' is not in the pre-trained Word2Vec model.\n",
            "Words similar to 'capture': [('capturing', 0.7563897371292114), ('captured', 0.7155306935310364), ('captures', 0.6099075078964233), ('Capturing', 0.6023245453834534), ('recapture', 0.5498639941215515), ('Capture', 0.5493018627166748), ('nab', 0.4941576421260834), ('Captured', 0.45745959877967834), ('apprehend', 0.4357919692993164), ('seize', 0.4338296055793762)]\n",
            "Words similar to 'semantic': [('semantics', 0.6644964814186096), ('Semantic', 0.6464474201202393), ('contextual', 0.5909127593040466), ('meta', 0.5905876755714417), ('ontology', 0.5880525708198547), ('Semantic_Web', 0.5612248778343201), ('semantically', 0.5600483417510986), ('microformat', 0.5582399368286133), ('inferencing', 0.5541478991508484), ('terminological', 0.5533202290534973)]\n",
            "Words similar to 'meanings': [('grammatical_constructions', 0.594986081123352), ('idioms', 0.5938195586204529), ('connotations', 0.5836683511734009), ('symbolic_meanings', 0.5806494951248169), ('meaning', 0.5785343647003174), ('literal_meanings', 0.5743482112884521), ('denotative', 0.5730364918708801), ('phrasal_verbs', 0.5697917342185974), ('contexts', 0.5609514713287354), ('adjectives_adverbs', 0.5569407343864441)]\n",
            "'of' is not in the pre-trained Word2Vec model.\n",
            "Words similar to 'words': [('phrases', 0.7100036144256592), ('phrase', 0.6408688426017761), ('Words', 0.6160537600517273), ('word', 0.5864380598068237), ('adjectives', 0.5812757015228271), ('uttered', 0.5724518299102783), ('plate_umpire_Tony_Randozzo', 0.5642045140266418), ('expletives', 0.5539036989212036), ('Mayor_Cirilo_Pena', 0.553884744644165), ('Tele_prompter', 0.5441114902496338)]\n",
            "Words similar to 'in': [('inthe', 0.5891957879066467), ('where', 0.5662435293197632), ('the', 0.5429296493530273), ('In', 0.5415117144584656), ('during', 0.5188906192779541), ('iin', 0.48737412691116333), ('at', 0.484235554933548), ('from', 0.48268404603004456), ('outside', 0.47092658281326294), ('for', 0.4566476047039032)]\n",
            "'a' is not in the pre-trained Word2Vec model.\n",
            "Words similar to 'vector': [('vectors', 0.750322163105011), ('adeno_associated_viral_AAV', 0.5999537110328674), ('bitmap_graphics', 0.5428463220596313), ('Sindbis', 0.5353653430938721), ('bitmap_images', 0.5318013429641724), ('signal_analyzer_VSA', 0.5276671051979065), ('analyzer_VNA', 0.5184376239776611), ('vectorial', 0.5084835886955261), ('nonviral_gene_therapy', 0.5036363005638123), ('shellcode', 0.5015827417373657)]\n",
            "Words similar to 'space': [('spaces', 0.6570690870285034), ('music_concept_ShockHound', 0.5850345492362976), ('Shuttle_docks', 0.5566749572753906), ('Space', 0.5478203296661377), ('Soviet_Union_Yuri_Gagarin', 0.5417766571044922), ('Shuttle_Discovery_blasts', 0.5352603197097778), ('Shuttle_Discovery_docks', 0.534925103187561), ('Shuttle_Endeavour_undocks', 0.532420814037323), ('Shuttle_Discovery_arrives', 0.5323426723480225), ('Shuttle_undocks', 0.523307740688324)]\n",
            "'.' is not in the pre-trained Word2Vec model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4 : Case Study"
      ],
      "metadata": {
        "id": "mGgOQOVcQadX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Initialize NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Function to perform semantic analysis\n",
        "def semantic_analysis(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    # Synonyms generation\n",
        "    synonyms = set()\n",
        "    for token in lemmatized_tokens:\n",
        "        for syn in wordnet.synsets(token):\n",
        "            for lemma in syn.lemmas():\n",
        "                synonyms.add(lemma.name())\n",
        "    return list(synonyms)\n",
        "# Example customer queries\n",
        "customer_queries = [\n",
        "\"I received a damaged product. Can I get a refund?\",\n",
        "\"I'm having trouble accessing my account.\",\n",
        "\"How can I track my order status?\",\n",
        "\"The item I received doesn't match the description.\",\n",
        "\"Is there a discount available for bulk orders?\"\n",
        "]\n",
        "# Semantic analysis for each query\n",
        "for query in customer_queries:\n",
        "    print(\"Customer Query:\", query)\n",
        "    synonyms = semantic_analysis(query)\n",
        "    print(\"Semantic Analysis (Synonyms):\", synonyms)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNW7AsJwQcPl",
        "outputId": "bc3e991f-16fb-40cf-8a81-c403e66220cf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Query: I received a damaged product. Can I get a refund?\n",
            "Semantic Analysis (Synonyms): ['vex', 'dumbfound', 'become', 'cause', 'return', 'stupefy', 'start', 'grow', 'receive', 'pick_up', 'nonplus', 'acquire', 'received', 'perplex', 'standard', 'drive', 'meet', 'ware', 'mathematical_product', 'fix', 'fetch', 'repay', 'puzzle', 'pose', 'bewilder', 'experience', 'merchandise', 'baffle', 'invite', 'come', 'damaged', 'bring', 'induce', 'contract', 'draw', 'amaze', 'set_about', 'have', 'bugger_off', 'production', 'capture', 'go', 'bring_forth', 'arrive', 'mystify', 'sire', 'produce', 'pay_back', 'stick', 'welcome', 'buzz_off', \"get_under_one's_skin\", 'get_down', 'fuck_off', 'arrest', 'commence', 'repayment', 'engender', 'develop', 'take_in', 'start_out', 'beat', 'let', 'begin', 'aim', 'gravel', 'Cartesian_product', 'flummox', 'mother', 'take', 'damage', 'pay_off', 'product', 'convey', 'catch', 'set_out', 'find', 'generate', 'stimulate', 'scram', 'beget', 'intersection', 'make', 'incur', 'get', 'obtain', 'give_back', 'encounter', 'sustain', 'suffer', 'discredited', 'refund', 'father']\n",
            "\n",
            "\n",
            "Customer Query: I'm having trouble accessing my account.\n",
            "Semantic Analysis (Synonyms): ['disturb', 'calculate', 'write_up', 'upset', 'score', 'fuss', 'explanation', 'disoblige', 'history', 'unhinge', 'account', 'story', 'cark', 'chronicle', 'hassle', 'accounting', 'describe', 'put_out', 'trouble', 'pain', 'report', 'trouble_oneself', 'access', 'discommode', 'incommode', 'ail', 'answer_for', 'perturb', 'problem', 'news_report', 'business_relationship', 'difficulty', 'bill', 'account_statement', 'get_at', 'inconvenience_oneself', 'invoice', 'disquiet', 'distract', 'inconvenience', 'worry', 'disorder', 'bother']\n",
            "\n",
            "\n",
            "Customer Query: How can I track my order status?\n",
            "Semantic Analysis (Synonyms): ['racetrack', 'social_club', 'path', 'tag', 'status', 'say', 'cross', 'rate', 'go_after', 'Order', 'ordain', 'monastic_order', 'rank', 'grade', 'put', 'arrange', 'edict', 'tell', 'runway', 'purchase_order', 'ordering', 'rails', 'Holy_Order', 'cart_track', 'rail', 'raceway', 'consecrate', 'order_of_magnitude', 'fiat', 'condition', 'get_across', 'rules_of_order', 'chase_after', 'regularize', 'rescript', 'enjoin', 'traverse', 'ordination', 'dictate', 'parliamentary_procedure', 'guild', 'parliamentary_law', 'lodge', 'cover', 'data_track', 'range', 'cut_across', 'place', 'govern', 'track', 'dog', 'set_up', 'trail', 'cartroad', 'racecourse', 'give_chase', 'prescribe', 'regularise', 'cut', 'pass_over', 'get_over', 'caterpillar_tread', 'orderliness', 'regulate', 'running', 'course', 'chase', 'order', 'society', 'ordinate', 'position', 'tail', 'gild', 'lead', 'caterpillar_track', 'cut_through', 'decree', 'club']\n",
            "\n",
            "\n",
            "Customer Query: The item I received doesn't match the description.\n",
            "Semantic Analysis (Synonyms): ['oppose', 'friction_match', 'compeer', 'mates', 'mate', 'experience', 'correspond', 'lucifer', 'catch', 'welcome', 'token', 'find', 'invite', 'touch', 'couple', 'receive', 'pick_up', 'match', 'verbal_description', 'detail', 'equate', 'peer', 'equal', 'fit', 'pair', 'equalise', 'tally', 'cope_with', 'pit', 'play_off', 'received', 'twin', 'take_in', 'item', 'have', 'standard', 'description', 'jibe', 'equalize', 'agree', 'gibe', 'incur', 'get', 'obtain', 'meet', 'particular', 'point', 'encounter', 'rival', 'check']\n",
            "\n",
            "\n",
            "Customer Query: Is there a discount available for bulk orders?\n",
            "Semantic Analysis (Synonyms): ['discount_rate', 'dismiss', 'order_of_magnitude', 'social_club', 'fiat', 'majority', 'usable', 'orderliness', 'regulate', 'place', 'rules_of_order', 'govern', 'mass', 'volume', 'say', 'regularize', 'rate', 'discount', 'price_reduction', 'ignore', 'bulge', 'rescript', 'bank_discount', 'rebate', 'order', 'brush_aside', 'Order', 'monastic_order', 'ordain', 'set_up', 'enjoin', 'rank', 'society', 'ordinate', 'arrange', 'put', 'edict', 'tell', 'grade', 'ordination', 'dictate', 'purchase_order', 'ordering', 'guild', 'useable', 'deduction', 'gild', 'parliamentary_procedure', 'parliamentary_law', 'Holy_Order', 'disregard', 'available', 'lodge', 'bulk', 'uncommitted', 'prescribe', 'decree', 'club', 'regularise', 'range', 'push_aside', 'consecrate', 'brush_off']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 Sentiment Analysis"
      ],
      "metadata": {
        "id": "7KJPBGzXQh2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install scikit-learn\n",
        "!pip install nltk\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.corpus import movie_reviews # Sample dataset from NLTK\n",
        "# Download NLTK resources (run only once if not downloaded)\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "# Load the movie_reviews dataset\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        " for category in movie_reviews.categories()\n",
        " for fileid in movie_reviews.fileids(category)]\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(documents, columns=['text', 'sentiment'])\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.2,\n",
        "random_state=42)\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.apply(' '.join))\n",
        "# Initialize SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train_tfidf, y_train)\n",
        "# Transform the test data\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test.apply(' '.join))\n",
        "# Predict on the test data\n",
        "y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "# Display classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBcUUe80QoBF",
        "outputId": "eb8e7158-58b6-45b8-f4e0-436a999392ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.83      0.85      0.84       199\n",
            "         pos       0.85      0.82      0.84       201\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5 : Case Study"
      ],
      "metadata": {
        "id": "4U1A7U9qQ5hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "# Download NLTK resources (only required once)\n",
        "nltk.download('vader_lexicon')\n",
        "# Sample reviews\n",
        "reviews = [\n",
        "\"This product is amazing! I love it.\",\n",
        "\"The product was good, but the packaging was damaged.\",\n",
        "\"Very disappointing experience. Would not recommend.\",\n",
        "\"Neutral feedback on the product.\",\n",
        "]\n",
        "# Initialize Sentiment Intensity Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "# Analyze sentiment for each review\n",
        "for review in reviews:\n",
        "    print(\"Review:\", review)\n",
        "    scores = sid.polarity_scores(review)\n",
        "    print(\"Sentiment:\", end=' ')\n",
        "    if scores['compound'] > 0.05:\n",
        "        print(\"Positive\")\n",
        "    elif scores['compound'] < -0.05:\n",
        "        print(\"Negative\")\n",
        "    else:\n",
        "        print(\"Neutral\")\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQRUeT1GQ7Px",
        "outputId": "7f49d625-5f0f-40f5-89fc-c6588d87978b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: This product is amazing! I love it.\n",
            "Sentiment: Positive\n",
            "Review: The product was good, but the packaging was damaged.\n",
            "Sentiment: Negative\n",
            "Review: Very disappointing experience. Would not recommend.\n",
            "Sentiment: Negative\n",
            "Review: Neutral feedback on the product.\n",
            "Sentiment: Neutral\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 Part of Speech Tagging"
      ],
      "metadata": {
        "id": "CTYBRLojCzmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBBpMyMPFKG6",
        "outputId": "cbcc6baa-f39d-4a8b-e2ba-7aabda3814c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK (if not already installed)\n",
        "#!pip install nltk\n",
        "# Import necessary libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Sample text for POS tagging\n",
        "text = \"Parts of speech tagging helps to understand the function of each word in a sentence.\"\n",
        "# Tokenize the text into words\n",
        "tokens = nltk.word_tokenize(text)\n",
        "# Perform POS tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "# Display the POS tags\n",
        "print(\"POS tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLKOgycYC3Px",
        "outputId": "9df7c397-468f-4e4b-e0e4-d1b87a4eb64a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags: [('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('tagging', 'VBG'), ('helps', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('the', 'DT'), ('function', 'NN'), ('of', 'IN'), ('each', 'DT'), ('word', 'NN'), ('in', 'IN'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6 : Case Study"
      ],
      "metadata": {
        "id": "zNcYyKt3DnPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "# Download NLTK resources (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "def pos_tagging(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    tagged_tokens = []\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        tagged_tokens.extend(nltk.pos_tag(tokens))\n",
        "    return tagged_tokens\n",
        "def main():\n",
        "    # Example news article\n",
        "    article_text = \"\"\"\n",
        "Manchester United secured a 3-1 victory over Chelsea in yesterday's\n",
        "match.\n",
        "Goals from Rashford, Greenwood, and Fernandes sealed the win for\n",
        "United.\n",
        "Chelsea's only goal came from Pulisic in the first half.\n",
        "The victory boosts United's chances in the Premier League title\n",
        "race.\n",
        "\"\"\"\n",
        "    tagged_tokens = pos_tagging(article_text)\n",
        "    print(\"Original Article Text:\\n\", article_text)\n",
        "    print(\"\\nParts of Speech Tagging:\")\n",
        "    for token, pos_tag in tagged_tokens:\n",
        "        print(f\"{token}: {pos_tag}\")\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gl5WeXEuDpth",
        "outputId": "3598cdc2-06e4-451e-e7fa-4c0cc21aaee1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Article Text:\n",
            " \n",
            "Manchester United secured a 3-1 victory over Chelsea in yesterday's\n",
            "match.\n",
            "Goals from Rashford, Greenwood, and Fernandes sealed the win for\n",
            "United.\n",
            "Chelsea's only goal came from Pulisic in the first half.\n",
            "The victory boosts United's chances in the Premier League title\n",
            "race.\n",
            "\n",
            "\n",
            "Parts of Speech Tagging:\n",
            "Manchester: NNP\n",
            "United: NNP\n",
            "secured: VBD\n",
            "a: DT\n",
            "3-1: JJ\n",
            "victory: NN\n",
            "over: IN\n",
            "Chelsea: NNP\n",
            "in: IN\n",
            "yesterday's: NNP\n",
            "match: NN\n",
            ".: .\n",
            "Goals: NNS\n",
            "from: IN\n",
            "Rashford: NNP\n",
            ",: ,\n",
            "Greenwood: NNP\n",
            ",: ,\n",
            "and: CC\n",
            "Fernandes: NNP\n",
            "sealed: VBD\n",
            "the: DT\n",
            "win: NN\n",
            "for: IN\n",
            "United: NNP\n",
            ".: .\n",
            "Chelsea: NN\n",
            "'s: POS\n",
            "only: JJ\n",
            "goal: NN\n",
            "came: VBD\n",
            "from: IN\n",
            "Pulisic: NNP\n",
            "in: IN\n",
            "the: DT\n",
            "first: JJ\n",
            "half: NN\n",
            ".: .\n",
            "The: DT\n",
            "victory: NN\n",
            "boosts: VBZ\n",
            "United: NNP\n",
            "'s: POS\n",
            "chances: NNS\n",
            "in: IN\n",
            "the: DT\n",
            "Premier: NNP\n",
            "League: NNP\n",
            "title: NN\n",
            "race: NN\n",
            ".: .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7 Chunking"
      ],
      "metadata": {
        "id": "GnGMlI0kD4bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk import RegexpParser\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "# Download NLTK resources (run only once if not downloaded)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(sentence)\n",
        "# POS tagging\n",
        "tagged = pos_tag(tokens)\n",
        "# Define a chunk grammar using regular expressions\n",
        "# NP (noun phrase) chunking: \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "# This grammar captures optional determiner (DT), adjectives (JJ), and nouns (NN) as a noun phrase\n",
        "chunk_grammar = r\"\"\"\n",
        " NP: {<DT>?<JJ>*<NN>}\n",
        "\"\"\"\n",
        "# Create a chunk parser with the defined grammar\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "# Parse the tagged sentence to extract chunks\n",
        "chunks = chunk_parser.parse(tagged)\n",
        "# Display the chunks\n",
        "for subtree in chunks.subtrees():\n",
        " if subtree.label() == 'NP': # Print only noun phrases\n",
        "   print(subtree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9WpM5QTD51Z",
        "outputId": "f2da6622-184b-4bd5-83fa-c750456c0bcc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "(NP The/DT quick/JJ brown/NN)\n",
            "(NP fox/NN)\n",
            "(NP the/DT lazy/JJ dog/NN)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7 : Case Study"
      ],
      "metadata": {
        "id": "q-o866uyEMGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "# Set NLTK data path\n",
        "nltk.data.path.append(\"/usr/local/share/nltk_data\")\n",
        "# Download the 'punkt' tokenizer model\n",
        "nltk.download('punkt')\n",
        "# Download the 'averaged_perceptron_tagger' model\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Sample text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "# Perform part-of-speech tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "# Define chunk grammar\n",
        "chunk_grammar = r\"\"\"\n",
        "NP: {<DT>?<JJ>*<NN>} # Chunk sequences of DT, JJ, NN\n",
        "\"\"\"\n",
        "# Create chunk parser\n",
        "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
        "# Apply chunking\n",
        "chunked_text = chunk_parser.parse(pos_tags)\n",
        "# Extract noun phrases\n",
        "noun_phrases = []\n",
        "for subtree in chunked_text.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "    noun_phrases.append(' '.join(word for word, tag in\n",
        "subtree.leaves()))\n",
        "# Output\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Noun Phrases:\")\n",
        "for phrase in noun_phrases:\n",
        "    print(\"-\", phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW5T6FHfENu5",
        "outputId": "33d1b677-5962-4a78-ca17-161c328b4233"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: The quick brown fox jumps over the lazy dog.\n",
            "Noun Phrases:\n",
            "- The quick brown\n",
            "- fox\n",
            "- the lazy dog\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    }
  ]
}